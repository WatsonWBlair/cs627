{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from transformers.modeling_outputs import BaseModelOutput\n",
    "import numpy as np\n",
    "import logging\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "# Set up logging for better visibility\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Loading BART model 'facebook/bart-base' to device: cpu\n"
     ]
    }
   ],
   "source": [
    "# --- Global Configuration ---\n",
    "MODEL_NAME = \"facebook/bart-base\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Initialize Tokenizer and Model\n",
    "try:\n",
    "    logging.info(f\"Loading BART model '{MODEL_NAME}' to device: {DEVICE}\")\n",
    "    tokenizer = BartTokenizer.from_pretrained(MODEL_NAME)\n",
    "    model = BartForConditionalGeneration.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error loading model: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 1) TEXT → ENCODER CONTEXT\n",
    "# ---------------------------------------------------------------------------\n",
    "def encode_text_to_context(\n",
    "    text_sentence: str,\n",
    "    *,\n",
    "    max_length: int = 1024,\n",
    "    device: Optional[str] = None\n",
    ") -> Tuple[BaseModelOutput, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Encodes an input sentence into the BART encoder and returns the full encoder context.\n",
    "\n",
    "    Args:\n",
    "        text_sentence (str):\n",
    "            Input English text that will be encoded by the BART encoder.\n",
    "\n",
    "        max_length (int, optional):\n",
    "            Maximum token length for the tokenizer. Defaults to 1024.\n",
    "\n",
    "        device (str, optional):\n",
    "            Device to run the model on (\"cpu\" or \"cuda\").\n",
    "            If None, uses global DEVICE.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[BaseModelOutput, torch.Tensor]:\n",
    "            encoder_output:\n",
    "                The full hidden-state sequence from the encoder.\n",
    "            attention_mask:\n",
    "                Attention mask corresponding to the input sequence.\n",
    "\n",
    "    Notes:\n",
    "        - If the user passes an empty string, a zero placeholder encoder\n",
    "          output is returned to avoid runtime crashes.\n",
    "        - This function only performs encoding; no pooling or decoding.\n",
    "    \"\"\"\n",
    "    device = device or DEVICE\n",
    "\n",
    "    if not text_sentence:\n",
    "        logging.warning(\"Input sentence is empty. Returning zero tensors.\")\n",
    "        empty = BaseModelOutput(\n",
    "            last_hidden_state=torch.zeros((1, 1, model.config.hidden_size), device=device)\n",
    "        )\n",
    "        return empty, torch.zeros((1, 1), device=device)\n",
    "\n",
    "    try:\n",
    "        inputs = tokenizer(\n",
    "            text_sentence,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=max_length,\n",
    "            truncation=True\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            encoder_output = model.model.encoder(**inputs)\n",
    "\n",
    "        logging.info(\n",
    "            f\"Encoded text into sequence context: {encoder_output.last_hidden_state.shape}\"\n",
    "        )\n",
    "\n",
    "        return encoder_output, inputs[\"attention_mask\"]\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during encoding: {e}\")\n",
    "        empty = BaseModelOutput(\n",
    "            last_hidden_state=torch.zeros((1, 1, model.config.hidden_size), device=device)\n",
    "        )\n",
    "        return empty, torch.zeros((1, 1), device=device)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 2) ENCODER CONTEXT → SONAR VECTOR\n",
    "# ---------------------------------------------------------------------------\n",
    "def calculate_sonar_vector(\n",
    "    encoder_output: BaseModelOutput,\n",
    "    *,\n",
    "    pooling: str = \"mean\"\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Converts the encoder's hidden-state sequence into a fixed-size vector.\n",
    "\n",
    "    Args:\n",
    "        encoder_output (BaseModelOutput):\n",
    "            The full sequence output from the BART encoder.\n",
    "\n",
    "        pooling (str, optional):\n",
    "            How to compress the sequence into one vector:\n",
    "                \"mean\" → mean pool across sequence length.\n",
    "                \"cls\"  → use token at position 0.\n",
    "            Default: \"mean\"\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor:\n",
    "            A 768-dimensional latent vector representing the entire input sentence.\n",
    "    \"\"\"\n",
    "    if pooling == \"cls\":\n",
    "        return encoder_output.last_hidden_state[:, 0, :]\n",
    "\n",
    "    return torch.mean(encoder_output.last_hidden_state, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 3) ENCODER CONTEXT → DECODED TEXT\n",
    "# ---------------------------------------------------------------------------\n",
    "def decode_context_to_text(\n",
    "    encoder_output: BaseModelOutput,\n",
    "    attention_mask: torch.Tensor,\n",
    "    *,\n",
    "    max_new_tokens: int = 40,\n",
    "    num_beams: int = 4,\n",
    "    device: Optional[str] = None\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Decodes text directly from a BART encoder output using model.generate().\n",
    "\n",
    "    Args:\n",
    "        encoder_output (BaseModelOutput):\n",
    "            Encoder hidden-state sequence to be decoded.\n",
    "\n",
    "        attention_mask (torch.Tensor):\n",
    "            Attention mask for the original input sequence.\n",
    "\n",
    "        max_new_tokens (int, optional):\n",
    "            Maximum generated tokens. Defaults to 40.\n",
    "\n",
    "        num_beams (int, optional):\n",
    "            Number of beams for beam-search decoding. Defaults to 4.\n",
    "\n",
    "        device (str, optional):\n",
    "            Device for inference. Default uses global DEVICE.\n",
    "\n",
    "    Returns:\n",
    "        str:\n",
    "            The decoded text generated from the latent sequence context.\n",
    "\n",
    "    Notes:\n",
    "        - The caller *must* ensure the encoder_output and attention_mask come\n",
    "          from the same input batch.\n",
    "        - This function **expects valid encoder_output**, not latent vectors.\n",
    "    \"\"\"\n",
    "    device = device or DEVICE\n",
    "\n",
    "    try:\n",
    "        generated_ids = model.generate(\n",
    "            encoder_outputs=encoder_output,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=max_new_tokens,\n",
    "            num_beams=num_beams,\n",
    "            early_stopping=True,\n",
    "        )\n",
    "\n",
    "        decoded_text = tokenizer.decode(\n",
    "            generated_ids.squeeze(),\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        logging.info(f\"Decoded text: '{decoded_text}'\")\n",
    "        return decoded_text\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during decoding: {e}\")\n",
    "        return \"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Encoded text into sequence context: torch.Size([1, 21, 768])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- SONAR Latent Context Test (cpu) ---\n",
      "\n",
      "[INPUT TEXT]: \"The team will evaluate the performance of models using the SuperGLU benchmark for NLP tasks.\"\n",
      "\n",
      "--- Running Encoder (Text -> Full Latent Context) ---\n",
      "Latent Sequence Shape: torch.Size([1, 21, 768])\n",
      "SONAR Vector Shape: torch.Size([1, 768])\n",
      "SONAR Vector (first 5): [ 0.22488703 -0.03958526  0.01248108  0.06425868 -0.03248952]\n",
      "\n",
      "--- Running Decoder (Context -> Text) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Decoded text: 'The team will evaluate the performance of models using the SuperGLU benchmark for NLP tasks.'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[DECODED TEXT]: \"The team will evaluate the performance of models using the SuperGLU benchmark for NLP tasks.\"\n",
      "\n",
      "--- Test Complete ---\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Example usage\n",
    "# ---------------------------------------------------------------------------\n",
    "if __name__ == '__main__':\n",
    "    print(f\"--- SONAR Latent Context Test ({DEVICE}) ---\")\n",
    "\n",
    "    input_sentence = \"The team will evaluate the performance of models using the SuperGLU benchmark for NLP tasks.\"\n",
    "    print(f\"\\n[INPUT TEXT]: \\\"{input_sentence}\\\"\")\n",
    "\n",
    "    print(\"\\n--- Running Encoder (Text -> Full Latent Context) ---\")\n",
    "    encoder_output, attention_mask = encode_text_to_context(input_sentence)\n",
    "\n",
    "    sonar_vector = calculate_sonar_vector(encoder_output)\n",
    "\n",
    "    print(f\"Latent Sequence Shape: {encoder_output.last_hidden_state.shape}\")\n",
    "    print(f\"SONAR Vector Shape: {sonar_vector.shape}\")\n",
    "    print(f\"SONAR Vector (first 5): {sonar_vector[0, :5].cpu().numpy()}\")\n",
    "\n",
    "    print(\"\\n--- Running Decoder (Context -> Text) ---\")\n",
    "    decoded_output = decode_context_to_text(encoder_output, attention_mask)\n",
    "\n",
    "    print(f\"\\n[DECODED TEXT]: \\\"{decoded_output}\\\"\")\n",
    "    print(\"\\n--- Test Complete ---\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
